{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT & SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics, Dates, Gadfly, LinearAlgebra, Distributions, Random, ScikitLearn, GLM, Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions globales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction générique pour former un dataframe contenant les variables explicatives\n",
    "- Array of data contient les dataframes des va explicatives\n",
    "- list_of_va contient le type des données i.e [\"sum\" \"max\" ...]\n",
    "- surverse contient les données de surverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function createDataEx(array_of_data, list_of_va, dates)\n",
    "    df = DataFrame(date = dates)\n",
    "    \n",
    "    for va in 1:length(list_of_va)\n",
    "        array = array_of_data[va]\n",
    "        McTavish = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        Bellevue = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        Assomption = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        Trudeau = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        StHubert = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        \n",
    "        for i=1:size(df,1)\n",
    "            ind = findfirst(array[:,:date] .== df[i,:date])\n",
    "            McTavish[i] = array[ind,:McTavish]\n",
    "            Bellevue[i] = array[ind,:Bellevue]\n",
    "            Assomption[i] = array[ind,:Assomption]\n",
    "            Trudeau[i] = array[ind,:Trudeau]\n",
    "            StHubert[i] = array[ind,:StHubert]\n",
    "        end\n",
    "        \n",
    "        df[!,Symbol(list_of_va[va] * \"McTavish\")] = McTavish\n",
    "        df[!,Symbol(list_of_va[va] * \"Bellevue\")] = Bellevue   \n",
    "        df[!,Symbol(list_of_va[va] * \"Assomption\")] = Assomption   \n",
    "        df[!,Symbol(list_of_va[va] * \"Trudeau\")] = Trudeau   \n",
    "        df[!,Symbol(list_of_va[va] * \"StHubert\")] = StHubert\n",
    "    end\n",
    "    \n",
    "    return df\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction pour recuperer seulement les donnees d'un ouvrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function getOuvrage(data, ouvrage)\n",
    "    return filter(row -> row.NO_OUVRAGE == ouvrage, data)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction pour recuperer seulement les donnees contenu aux bonnes dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function getDataFromDates(df, dates)\n",
    "    return filter(row -> row.date in dates, df)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction pour la conversion d'un dataframe en matrix/array\n",
    "- On ignore les dates et les ouvrages si necessaire\n",
    "- les modeles scikit requierent des arrays et non des dataframes, alors cette fonction nous permettera des les utiliser plus aisément"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function toArray(df::DataFrame)\n",
    "    \n",
    "    if :NO_OUVRAGE in names(df)\n",
    "        return convert(Matrix, df[:, Not([:date, :NO_OUVRAGE])])\n",
    "    elseif :date in names(df)\n",
    "        return convert(Matrix, df[:, Not(:date)])\n",
    "    else\n",
    "        return convert(Matrix, df)\n",
    "    end\n",
    "end\n",
    "function toArray(df::DataFrameRow)\n",
    "    \n",
    "    if :NO_OUVRAGE in names(df)\n",
    "        return convert(Vector, df[Not([:date, :NO_OUVRAGE])])\n",
    "    elseif :date in names(df)\n",
    "        return convert(Vector, df[Not(:date)])\n",
    "    else\n",
    "        return convert(Vector, df)\n",
    "    end\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    splitdataframe(df::DataFrame, p::Real)\n",
    "\n",
    "Partitionne en un ensemble d'entraînement et un ensemble de validation un DataFrame.\n",
    "\n",
    "### Arguments\n",
    "- `df::DataFrame` : Un DataFrame\n",
    "- `p::Real` : La proportion (entre 0 et 1) de données dans l'ensemble d'entraînement.\n",
    "\n",
    "### Détails\n",
    "\n",
    "La fonction renvoie deux DataFrames, un pour l'ensemble d'entraînement et l'autre pour l'ensemble de validation.\n",
    "\n",
    "### Exemple\n",
    "\n",
    "\\```\n",
    " julia> splitdataframe(df, p.7)\n",
    "\\```\n",
    "\n",
    "\"\"\"\n",
    "function splitdataframe(df::DataFrame, p::Real)\n",
    "   @assert 0 <= p <= 1 \n",
    "    \n",
    "    n = size(df,1)\n",
    "    \n",
    "    ind = shuffle(1:n)\n",
    "    \n",
    "    threshold = Int64(round(n*p))\n",
    "    \n",
    "    indTrain = sort(ind[1:threshold])\n",
    "    \n",
    "    indTest = setdiff(1:n,indTrain)\n",
    "    \n",
    "    dfTrain = df[indTrain,:]\n",
    "    dfTest = df[indTest,:]\n",
    "    \n",
    "    return dfTrain, dfTest\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture des fichiers des variables explicatives et des surverses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_max = CSV.read(\"x_train_max.csv\");\n",
    "x_pred_max = CSV.read(\"x_pred_max.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sum = CSV.read(\"x_train_sum.csv\");\n",
    "x_pred_sum = CSV.read(\"x_pred_sum.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum_last_2 = CSV.read(\"sum_day_last_2.csv\")\n",
    "x_train_sum_last_2 = filter(row -> Year(row[:date]) != Year(2019), x_sum_last_2)\n",
    "x_pred_sum_last_2  = filter(row -> Year(row[:date]) == Year(2019), x_sum_last_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_2 = CSV.read(\"maxPrecBy2hours.csv\")\n",
    "x_train_max_2 = filter(row -> Year(row[:date]) != Year(2019), x_max_2)\n",
    "x_pred_max_2  = filter(row -> Year(row[:date]) == Year(2019), x_max_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_3 = CSV.read(\"maxPrecBy3hours.csv\")\n",
    "x_train_max_3 = filter(row -> Year(row[:date]) != Year(2019), x_max_3)\n",
    "x_pred_max_3  = filter(row -> Year(row[:date]) == Year(2019), x_max_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_4 = CSV.read(\"maxPrecBy4hours.csv\")\n",
    "x_train_max_4 = filter(row -> Year(row[:date]) != Year(2019), x_max_4)\n",
    "x_pred_max_4  = filter(row -> Year(row[:date]) == Year(2019), x_max_4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_6 = CSV.read(\"maxPrecBy6hours.csv\")\n",
    "x_train_max_6 = filter(row -> Year(row[:date]) != Year(2019), x_max_6)\n",
    "x_pred_max_6  = filter(row -> Year(row[:date]) == Year(2019), x_max_6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_8 = CSV.read(\"maxPrecBy8hours.csv\")\n",
    "x_train_max_8 = filter(row -> Year(row[:date]) != Year(2019), x_max_8)\n",
    "x_pred_max_8  = filter(row -> Year(row[:date]) == Year(2019), x_max_8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_12 = CSV.read(\"maxPrecBy12hours.csv\")\n",
    "x_train_max_12 = filter(row -> Year(row[:date]) != Year(2019), x_max_12)\n",
    "x_pred_max_12  = filter(row -> Year(row[:date]) == Year(2019), x_max_12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Décision des variables explicatives considérées pour les régressions\n",
    "\n",
    "Suite à nos tests nous avons décidés de ne pas prendre les max des sommes des 6, 8 et 12 heures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = CSV.read(\"surverse_list.csv\");\n",
    "x_train = createDataEx(\n",
    "    [\n",
    "        x_train_max,\n",
    "        x_train_sum,\n",
    "        x_train_sum_last_2,\n",
    "        x_train_max_2,\n",
    "        x_train_max_3,\n",
    "        x_train_max_4,\n",
    "    ],\n",
    "    [\n",
    "        \"max\",\n",
    "        \"sum\",\n",
    "        \"sum2\",\n",
    "        \"max2\",\n",
    "        \"max3\",\n",
    "        \"max4\",\n",
    "    ], x_train_max_2[:, :date]\n",
    ")\n",
    "\n",
    "x_pred = createDataEx(\n",
    "    [\n",
    "        x_pred_max,\n",
    "        x_pred_sum,\n",
    "        x_pred_sum_last_2,\n",
    "        x_pred_max_2,\n",
    "        x_pred_max_3,\n",
    "        x_pred_max_4,\n",
    "    ],\n",
    "    [\n",
    "        \"max\",\n",
    "        \"sum\",\n",
    "        \"sum2\",\n",
    "        \"max2\",\n",
    "        \"max3\",\n",
    "        \"max4\",\n",
    "    ], x_pred_max_2[:, :date]\n",
    ")\n",
    "dropmissing!(x_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ouvrages observés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouvrages = [\"3260-01D\", \"3350-07D\", \"4240-01D\", \"4350-01D\", \"4380-01D\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse sur nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histogram(convert(Matrix, x_train[:, Not(:date)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on peut voir que les données sont reparties sur de très grandes distances et très peu uniformément\n",
    "- nous allons donc tenter d'avoir une répartition des données plus intéressante a l'aide d'une fonction logarithmique de la forme\n",
    "$ f(x) = ln(x+1) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = DataFrame([log.(col.+1) for col = eachcol(x_train[:, Not(:date)])])\n",
    "display(histogram(convert(Matrix, x_values)))\n",
    "display(describe(x_values, :mean, :std, :min, :q25, :median, :q75, :max))\n",
    "x_train_log = log.(x_train[:, Not(:date)] .+ 1)\n",
    "x_pred_log = log.(x_pred[:, Not(:date)] .+ 1)\n",
    "x_train_log.date = x_train.date\n",
    "x_pred_log.date = x_pred.date;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On constate qu'avec cette fonction nous obtenons une réparition beaucoup plus intéressante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicolinéarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_PCA = 10\n",
    "@sk_import decomposition : PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des composantes principales\n",
    "- L'analyse par les composantes principales est une bonne manière de réduire la multicolinéarité ce qui est important pour les modèles qui n'en font pas abstraction par eux-même"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# analyse en composantes principales pour les données non modifiées\n",
    "pca = PCA()\n",
    "x_train_matrix = pca.fit_transform(toArray(x_train))\n",
    "explained_variance = cumsum(pca.explained_variance_ratio_)\n",
    "display(Plots.plot(\n",
    "    1:length(explained_variance),\n",
    "    explained_variance,\n",
    "    ylims=(0, 1.0),\n",
    "    xlabel=\"Number of components\",\n",
    "    ylabel=\"Cumulative sum of explained Variance\"\n",
    "))\n",
    "\n",
    "x_train_pca = DataFrame(\n",
    "    date=x_train[!,:date],\n",
    "    x₁=x_train_matrix[:,1],\n",
    "    x₂=x_train_matrix[:,2],\n",
    "    x₃=x_train_matrix[:,3],\n",
    "    x₄=x_train_matrix[:,4],\n",
    "    x₅=x_train_matrix[:,5],\n",
    "    x₆=x_train_matrix[:,6],\n",
    "    x₇=x_train_matrix[:,7],\n",
    "    x₈=x_train_matrix[:,8],\n",
    "    x₉=x_train_matrix[:,9],\n",
    "    x₁₀=x_train_matrix[:,10],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse en composantes principales pour les données mise au logarithme\n",
    "pca = PCA()\n",
    "x_train_log_matrix = pca.fit_transform(toArray(x_train_log))\n",
    "explained_variance = cumsum(pca.explained_variance_ratio_)\n",
    "display(Plots.plot(\n",
    "    1:length(explained_variance),\n",
    "    explained_variance,\n",
    "    ylims=(0, 1.0),\n",
    "    xlabel=\"Number of components\",\n",
    "    ylabel=\"Cumulative sum of explained Variance\",\n",
    "    title=\"With ln(1+x)\"\n",
    "))\n",
    "\n",
    "x_train_log_pca = DataFrame(\n",
    "    date=x_train[!,:date],\n",
    "    x₁=x_train_log_matrix[:,1],\n",
    "    x₂=x_train_log_matrix[:,2],\n",
    "    x₃=x_train_log_matrix[:,3],\n",
    "    x₄=x_train_log_matrix[:,4],\n",
    "    x₅=x_train_log_matrix[:,5],\n",
    "    x₆=x_train_log_matrix[:,6],\n",
    "    x₇=x_train_log_matrix[:,7],\n",
    "    x₈=x_train_log_matrix[:,8],\n",
    "    x₉=x_train_log_matrix[:,9],\n",
    "    x₁₀=x_train_log_matrix[:,10],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Séparations des données pour les ensembles d'entrainement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = y_train\n",
    "X = x_pred\n",
    "X_PCA = x_train_pca\n",
    "X_LOG_PCA = x_train_log_pca\n",
    "X_BASE = x_train\n",
    "X_BASE_LOG = x_train_log;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function dataSplit(ouvrage, x_data)\n",
    "    y = getOuvrage(Y, ouvrage)\n",
    "    ytrain, ytest = splitdataframe(y, .80)\n",
    "\n",
    "    xtrain = getDataFromDates(x_data, ytrain.date)\n",
    "    ytrain = getDataFromDates(ytrain, xtrain.date)\n",
    "\n",
    "    xtest = getDataFromDates(x_data, ytest.date)\n",
    "    ytest = getDataFromDates(ytest, xtest.date)\n",
    "    \n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèles envisagés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@sk_import metrics: f1_score;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cette fonction nous permettra d'entrainer tous nos modèles en même temps avec les mêmes données et les comparer en fonction de leur précision noté avec la métrique f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_on_all_ouvrages(model_contructor::Function, data, cutoff)\n",
    "    pred = []\n",
    "    test = []\n",
    "    for ouvrage in ouvrages\n",
    "        x_train, x_test, y_train, y_test = dataSplit(ouvrage, data)\n",
    "        \n",
    "        x_train = toArray(x_train); x_test = toArray(x_test);\n",
    "        y_train = toArray(y_train); y_test = toArray(y_test);\n",
    "        \n",
    "        x_train = x_train[:, 1:cutoff]\n",
    "        x_test = x_test[:, 1:cutoff]\n",
    "        \n",
    "        model = model_contructor(x_train, y_train)\n",
    "        \n",
    "        prediction = model.predict(x_test);\n",
    "        \n",
    "        push!(test, y_test)\n",
    "        push!(pred, prediction)\n",
    "    end\n",
    "    \n",
    "    pred = collect(Iterators.flatten(pred))\n",
    "    test = collect(Iterators.flatten(test))\n",
    "    return f1_score(pred,test)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sk_import linear_model: LogisticRegression\n",
    "function modeleLogistique(x_train, y_train) \n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"liblinear\",\n",
    "        random_state=234,\n",
    "    )\n",
    "    model.fit(x_train, y_train);\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele de regression bayésienne naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sk_import naive_bayes: GaussianNB\n",
    "function modeleNaifGauss(xtrain, ytrain)\n",
    "    model = GaussianNB()\n",
    "    model.fit(xtrain, ytrain);\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele de l'arbre de Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sk_import tree: DecisionTreeClassifier\n",
    "function modeleArbreDecisif(x_train, y_train)\n",
    "    model = DecisionTreeClassifier(\n",
    "        random_state=234,\n",
    "    )\n",
    "    model.fit(x_train, y_train);\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificateur Ridge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce style de classification utilise une regression ridge pour traiter le problème. Toutes les variables sont converties en {-1, 1}. Le résultat est 0 si la regression donne un nombre négatif, et 1 si il est positif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sk_import linear_model: RidgeClassifier\n",
    "function modeleClassificationRidge(x_train, y_train)\n",
    "    model = RidgeClassifier(\n",
    "        alpha=.5,\n",
    "    )\n",
    "    model.fit(x_train, y_train);\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement de tous les modèles\n",
    "- Dans cette section nous allons entrainer nos modèles et les comparer pour choisir le meilleur\n",
    "- Pour être sur de ne pas entrainer nos modèles à prédire uniquement notre ensemble \"test\" (overfitting), nous employons une méthode qui entrainera plusieurs fois nos modèles avec différentes coupures de nos données pour les ensembles \"train\" et \"test\"\n",
    "- Cela nous permettra de calculer une moyenne des scores f1 de chaque entrainement et d'avoir une métrique beaucoup plus puissante, car nous pouvons assurer que le modèle testé est flexible et polyvalent \n",
    "- Lors de nos itérations, nous allons aussi entrainer nos modèles avec différents types de variables, soit avec et sans l'analyse des composantes principales et avec et sans l'application de la finction logarithmique aux données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La fonction ci-dessous nous calculera les moyennes des scores f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_avg_f1(m, iterations, data, cutoff)\n",
    "    score=[]\n",
    "    for i = 1:iterations\n",
    "        f1 = train_on_all_ouvrages(m, data, cutoff);\n",
    "        push!(score,f1)\n",
    "    end\n",
    "    f1 = mean(score)\n",
    "    f1 = round(f1, digits=4)\n",
    "    return f1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Le code ci-dessous effectue les itérations et les entrainements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ITERATIONS=20\n",
    "display(\"$(ITERATIONS)  iterations\")\n",
    "\n",
    "models = [\n",
    "    modeleLogistique,\n",
    "    modeleNaifGauss,\n",
    "    modeleArbreDecisif,\n",
    "    modeleClassificationRidge,\n",
    "];\n",
    "\n",
    "dataSets = [\n",
    "    (X_BASE, \"no pca without log\", \"no itr\"),\n",
    "    (X_BASE_LOG, \"no pca with log\", \"no itr\"),\n",
    "    (X_PCA, \"pca without log\", \"with itr\"),\n",
    "    (X_LOG_PCA, \"pca with log\", \"with itr\")\n",
    "];\n",
    "\n",
    "best_f1 = 0\n",
    "best_model = []\n",
    "\n",
    "for m in models\n",
    "    model_f1 = []\n",
    "    display(\"======= $(string(m)) ======\")\n",
    "    for data in dataSets\n",
    "        dataSet = data[1]; data_name = data[2]; is_itr = data[3];\n",
    "        display(\"------- $(data_name) ------\")\n",
    "        if is_itr == \"with itr\"\n",
    "            for pca_cutoff = 1:CUTOFF_PCA\n",
    "                f1 = get_avg_f1(m, ITERATIONS, dataSet, pca_cutoff)\n",
    "                push!(model_f1, f1)\n",
    "                display(f1)\n",
    "\n",
    "                if f1 > best_f1\n",
    "                    best_f1 = f1\n",
    "                    best_model = [f1, string(m), data_name, pca_cutoff]\n",
    "                end\n",
    "            end\n",
    "        else\n",
    "            n_cols = size(dataSet[:, Not(:date)], 2)\n",
    "            f1 = get_avg_f1(m, ITERATIONS, dataSet, n_cols)\n",
    "                push!(model_f1, f1)\n",
    "                display(f1)\n",
    "\n",
    "                if f1 > best_f1\n",
    "                    best_f1 = f1\n",
    "                    best_model = [f1, string(m), data_name, n_cols]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    display(\"MAX f1 for Model $(maximum(model_f1))\")\n",
    "end\n",
    "\n",
    "display(\"BEST MODEL IS $(best_model[2]), with f1: $(best_model[1]), dataset: $(best_model[3]), pca cutoff: $(best_model[4])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Meilleurs**\n",
    "\n",
    "Modele logistique .7174 avec pca sans log et cutoff 9\n",
    "\n",
    "Modele Naif Gauss .6988 avec pca avec log et cutoff 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreer les meilleurs modèles et les entrainer\n",
    "- À l'aide du test précédent, nous avons sélectionné les deux meilleurs modèles pour les 2 soumissions au concours et nous allons les réentrainer avec l'ensemble des données que nous avons à notre disposition\n",
    "- cela veut dire que nous ne faisont plus la séparation entre \"train\" et \"test\", nous utilisons toutes les données comme ensemble \"train\" puisque le kaggle contient l'ensemble \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function creerModeles(modele::Function, data, cutoff)\n",
    "    modeles = DataFrame(NO_OUVRAGE = [], modele = [])\n",
    "    \n",
    "    for ouvrage in ouvrages\n",
    "        y_train = getOuvrage(Y, ouvrage)\n",
    "\n",
    "        x_train = getDataFromDates(data, y_train.date)\n",
    "        y_train = getDataFromDates(y_train, x_train.date)\n",
    "\n",
    "        model = modele(toArray(x_train)[:, 1:cutoff], toArray(y_train));\n",
    "\n",
    "        push!(modeles, [ouvrage, model])\n",
    "    end\n",
    "    \n",
    "    return modeles\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logisitique\n",
    "cutoff = 9\n",
    "data = X_PCA\n",
    "\n",
    "modelesLogistiques = creerModeles(modeleLogistique, data, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naif Bayésien\n",
    "cutoff = 1\n",
    "data = X_LOG_PCA\n",
    "\n",
    "modelesNaifGauss = creerModeles(modeleNaifGauss, data, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbre décisif\n",
    "cutoff = 8\n",
    "data = X_PCA\n",
    "\n",
    "modelesArbreDecisif = creerModeles(modeleArbreDecisif, data, cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatter les predictions en consequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=9)\n",
    "x_pred_log_pca = pca.fit_transform(toArray(x_pred))\n",
    "x_pred_logistique = DataFrame(\n",
    "    date=x_pred[!,:date],\n",
    "    x₁=x_pred_log_pca[:,1],\n",
    "    x₂=x_pred_log_pca[:,2],\n",
    "    x₃=x_pred_log_pca[:,3],\n",
    "    x₄=x_pred_log_pca[:,4],\n",
    "    x₅=x_pred_log_pca[:,5],\n",
    "    x₆=x_pred_log_pca[:,6],\n",
    "    x₇=x_pred_log_pca[:,7],\n",
    "    x₈=x_pred_log_pca[:,8],\n",
    "    x₉=x_pred_log_pca[:,9],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=8)\n",
    "x_pred_arbre_pca = pca.fit_transform(toArray(x_pred))\n",
    "x_pred_arbre = DataFrame(\n",
    "    date=x_pred[!,:date],\n",
    "    x₁=x_pred_arbre_pca[:,1],\n",
    "    x₂=x_pred_arbre_pca[:,2],\n",
    "    x₃=x_pred_arbre_pca[:,3],\n",
    "    x₄=x_pred_arbre_pca[:,4],\n",
    "    x₅=x_pred_arbre_pca[:,5],\n",
    "    x₆=x_pred_arbre_pca[:,6],\n",
    "    x₇=x_pred_arbre_pca[:,7],\n",
    "    x₈=x_pred_arbre_pca[:,8],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred_log = log.(x_pred[:, Not(:date)] .+ 1)\n",
    "pca = PCA(n_components=1)\n",
    "x_pred_log_pca = pca.fit_transform(toArray(x_pred))\n",
    "x_pred_naif = DataFrame(\n",
    "    date=x_pred[!,:date],\n",
    "    x₁=x_pred_log_pca[:,1],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place des valeurs cherchees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = CSV.read(\"data/test.csv\")\n",
    "df = test[:, [:NO_OUVRAGE, :DATE]]\n",
    "rename!(df, :DATE=>:date)\n",
    "X_logistique = join(df, x_pred_logistique, on=:date)\n",
    "X_naif = join(df, x_pred_naif, on=:date)\n",
    "X_arbre = join(df, x_pred_arbre, on=:date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction des données\n",
    "- Cette fonction permet d'utiliser des modèles rapidement\n",
    "- Elle nous permettera donc d'effectuer les predictions facilement avec 5 modèles (1 par ouvrage) pour la même soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predictValues(dataframe, modeles)\n",
    "    predictions = []\n",
    "    for x in eachrow(dataframe)\n",
    "        modele = filter(row -> row.NO_OUVRAGE == x.NO_OUVRAGE, modeles).modele[1]\n",
    "        predictions = vcat(predictions, modele.predict([toArray(x)]))\n",
    "    end\n",
    "    return round.(predictions)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsLogit = predictValues(X_logistique, modelesLogistiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsNaif = predictValues(X_naif, modelesNaifGauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsArbre = predictValues(X_arbre, modelesArbreDecisif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre de valeurs prédites différentes entre les deux modèles sélectionnés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(abs.(predictionsLogit-predictionsNaif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(abs.(predictionsLogit-predictionsArbre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du fichier de prédictions pour soumettre sur Kaggle\n",
    "\n",
    "Dans ce cas-ci, nous prédirons une surverse avec une prediction logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du fichier sampleSubmission.csv pour soumettre sur Kaggle avec modèle naif bayésien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ID = test[:,:NO_OUVRAGE].*\"_\".*string.(test[:,:DATE])\n",
    "sampleSubmission = DataFrame(ID = ID, Surverse=predictionsNaif)\n",
    "CSV.write(\"submission_naif.csv\",sampleSubmission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du fichier sampleSubmission.csv pour soumettre sur Kaggle avec modèle logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = test[:,:NO_OUVRAGE].*\"_\".*string.(test[:,:DATE])\n",
    "sampleSubmission = DataFrame(ID = ID, Surverse=predictionsLogit)\n",
    "CSV.write(\"submission_logit.csv\",sampleSubmission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du fichier sampleSubmission.csv pour soumettre sur Kaggle avec modèle arbre décisif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = test[:,:NO_OUVRAGE].*\"_\".*string.(test[:,:DATE])\n",
    "sampleSubmission = DataFrame(ID = ID, Surverse=predictionsArbre)\n",
    "CSV.write(\"submission_logit.csv\",sampleSubmission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
