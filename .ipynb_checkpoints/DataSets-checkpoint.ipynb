{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoteBook To create all sorts of DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all new datasets in `data/new datasets/`\n",
    "\n",
    "All datasets should have their own subFolder with files \n",
    "\n",
    "`x_train` columns: All variable names with index 0-XXX in numbers (no arrays)\n",
    "\n",
    "`y_train` column: y column of 0/1 boolean values\n",
    "\n",
    "`x_pred` same as x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics, Dates, Gadfly, LinearAlgebra, Distributions, Random, ScikitLearn, GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions Globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    splitdataframe(df::DataFrame, p::Real)\n",
    "\n",
    "Partitionne en un ensemble d'entraînement et un ensemble de validation un DataFrame.\n",
    "\n",
    "### Arguments\n",
    "- `df::DataFrame` : Un DataFrame\n",
    "- `p::Real` : La proportion (entre 0 et 1) de données dans l'ensemble d'entraînement.\n",
    "\n",
    "### Détails\n",
    "\n",
    "La fonction renvoie deux DataFrames, un pour l'ensemble d'entraînement et l'autre pour l'ensemble de validation.\n",
    "\n",
    "### Exemple\n",
    "\n",
    "\\```\n",
    " julia> splitdataframe(df, p.7)\n",
    "\\```\n",
    "\n",
    "\"\"\"\n",
    "function splitdataframe(df::DataFrame, p::Real)\n",
    "   @assert 0 <= p <= 1 \n",
    "    \n",
    "    n = size(df,1)\n",
    "    \n",
    "    ind = shuffle(1:n)\n",
    "    \n",
    "    threshold = Int64(round(n*p))\n",
    "    \n",
    "    indTrain = sort(ind[1:threshold])\n",
    "    \n",
    "    indTest = setdiff(1:n,indTrain)\n",
    "    \n",
    "    dfTrain = df[indTrain,:]\n",
    "    dfTest = df[indTest,:]\n",
    "    \n",
    "    return dfTrain, dfTest\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données et nettoyage préliminaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des surverses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = CSV.read(\"./data/surverses.csv\", missingstring=\"-99999\")\n",
    "first(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données sur les surverses\n",
    "\n",
    "#### Extraction des surverses pour les mois de mai à octobre inclusivement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter(row -> month(row.DATE) > 4, data) \n",
    "data = filter(row -> month(row.DATE) < 11, data) \n",
    "first(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remplacement des valeurs *missing* dans la colonne :RAISON par \"Inconnue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raison = coalesce.(data[:,:RAISON],\"Inconnue\")\n",
    "data[!,:RAISON] = raison\n",
    "first(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exlusion des surverses coccasionnées par d'autres facteurs que les précipitations liquides\n",
    "\n",
    "Ces facteurs correspondent à : \n",
    "- la fonte de neige (F), \n",
    "- les travaux planifiés et entretien (TPL)\n",
    "- urgence (U)\n",
    "- autre (AUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter(row -> row.RAISON ∈ [\"P\",\"Inconnue\",\"TS\"], data) \n",
    "select!(data, [:NO_OUVRAGE, :DATE, :SURVERSE])\n",
    "first(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusion des lignes où :SURVERSE est manquante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surverse_df = dropmissing(data, disallowmissing=true)\n",
    "rename!(surverse_df, :DATE=>:date)\n",
    "first(surverse_df,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des précipitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CSV.read(\"data/precipitations.csv\",missingstring=\"-99999\")\n",
    "rename!(data, Symbol(\"St-Hubert\")=>:StHubert)\n",
    "first(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données sur les précipitations\n",
    "\n",
    "#### Extraction des précipitations des mois de mai à octobre inclusivement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter(row -> month(row.date) > 4, data) \n",
    "data = filter(row -> month(row.date) < 11, data) \n",
    "first(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Remplissage des données manquantes\n",
    "Nous allons tenter de remplir les données manquantes par des moyennes de précipitations lorsque les données sont inconnues pour 2 stations ou plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIST OF TECHNIQUES TO FILL MISSING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use ridge regression to fill missing values (*TODO*)\n",
    "- use mean of line to fill values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction Ridge pour trouver les missings values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour faire une regression ridge\n",
    "# Ressort le beta, m, et s\n",
    "function ridge(datas::DataFrame, station::Symbol)\n",
    "       \n",
    "    Train, Test = splitdataframe(datas, .75);\n",
    "    # Prétraitement des données\n",
    "    # Les variables avec les tildes correspondent à l'échantillon de test\n",
    "\n",
    "    X = convert(Matrix{Int64},Train[:,Not(station)])\n",
    "    m = mean(X, dims=1)\n",
    "    s = std(X, dims=1)\n",
    "    m[2] = 0\n",
    "    s[2] = 1\n",
    "    X = (X .- m) ./ s\n",
    "\n",
    "    X̃ = convert(Matrix{Int64},Test[:,Not(station)])\n",
    "    X̃ = (X̃ .- m) ./ s\n",
    "\n",
    "    y = convert(Vector{Int64}, Train[:,station])\n",
    "    m = mean(y)\n",
    "    s = std(y)\n",
    "    y = (y .- m) ./s\n",
    "\n",
    "    ỹ = convert(Vector{Int64}, Test[:,station])\n",
    "    ỹ = (ỹ .- m) ./s;\n",
    "\n",
    "    #On calcule ensuite le RMSE pour chacun des valeurs de lambda\n",
    "    RMSEs = DataFrame(λ=Float64[], RMSE=Float64[])\n",
    "\n",
    "    for λ in 0:1:10000\n",
    "   \n",
    "        β̂ = (X'X + λ*I)\\X'y\n",
    "    \n",
    "        ŷ = X̃*β̂\n",
    "    \n",
    "        ẽ = ỹ - ŷ\n",
    "    \n",
    "        RMSE = sqrt(dot(ẽ,ẽ)/length(ẽ))\n",
    "    \n",
    "        push!(RMSEs, [λ, RMSE])\n",
    "    \n",
    "    end\n",
    "    \n",
    "    # On trouve ensuite la valeure de lambda qui minimise le RMSE\n",
    "    _, ind = findmin(RMSEs[:,:RMSE])\n",
    "\n",
    "    λ̂ = RMSEs[ind,:λ]\n",
    "    \n",
    "    β̂ = (X'X + λ̂*I)\\X'y\n",
    "    \n",
    "    #TODO validate model and print value of validator R² ajuste\n",
    "    \n",
    "    #On peut alors calculer les y avec les betas trouver et l'echantillon de test\n",
    "    ŷ = X̃ * β̂\n",
    "    ŷ = round.((ŷ .* s) .+ m)\n",
    "    \n",
    "    # Calcul du R² ajusté\n",
    "\n",
    "    p = 4          # nombre de variables explicatives\n",
    "    n = length(ỹ)  # taille de l'échantillon\n",
    "\n",
    "    ỹ = (ỹ .* s) .+ m\n",
    "    ȳ = mean(ỹ)\n",
    "    e = ỹ - ŷ\n",
    "\n",
    "    SST = sum( (ỹ[i] - ȳ)^2 for i=1:n )  # variabilité totale\n",
    "    SSE = sum( e.^2 )                    # variabilité résiduelle\n",
    "\n",
    "    R2aj =  1 - SSE/SST * (n-1)/(n-p)\n",
    "    \n",
    "    println(\"Le R² ajuste du modele trouve pour la station de $(station) est $(R2aj)\")\n",
    "    \n",
    "    return β̂\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Replace ridge fonction with SciKit fonction (check R square if better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the data with no missing values to build ridge models\n",
    "full_data = dropmissing(data, disallowmissing=true)\n",
    "full_data = full_data[:,Not(:date)][:, Not(:heure)]\n",
    "size(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of betas by missing station\n",
    "betas = DataFrame(station = Symbol[], β = Array{Float64}[])\n",
    "for name in names(full_data)\n",
    "    β̂ = ridge(full_data, name)\n",
    "    push!(betas, [name, β̂])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling precipitation rows by doing mean of Stations per Hour when more than one missing value, else use Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(\"datasets/countMissing.jl\")\n",
    "include(\"datasets/meanLine.jl\")\n",
    "include(\"datasets/replaceMissing.jl\")\n",
    "precipitation_df = data[:,Not(:date)][:,Not(:heure)]\n",
    "for row in eachrow(precipitation_df)\n",
    "    nbMissing, ind = countMissing(row)\n",
    "    if(nbMissing == 1)\n",
    "        row[ind] = round((convert(Vector{Float64},row[Not(ind)])'*betas[:, :β][ind]))\n",
    "    end\n",
    "    # remplacer les lignes qui ont de 2 a 4 missing\n",
    "    if(nbMissing<5 && nbMissing>1)\n",
    "        replaceMissing(row,round(meanLine(row)))\n",
    "    end\n",
    "end\n",
    "precipitation_df.heure = data[:,:heure]\n",
    "precipitation_df.date = data[:,:date]\n",
    "precipitation_df = dropmissing(precipitation_df) # drop all missing\n",
    "CSV.write(\"data/new_datasets/precipitation_filed_mean_per_hour.csv\",precipitation_df)\n",
    "first(precipitation_df,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily sum as 5 Explicative Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precipitation_daily_sum = by(precipitation_df, :date,  McTavish = :McTavish=>sum, Bellevue = :Bellevue=>sum, \n",
    "   Assomption = :Assomption=>sum, Trudeau = :Trudeau=>sum, StHubert = :StHubert=>sum)\n",
    "last(precipitation_daily_sum ,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter out 2019 year fore prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_daily_sum_train = filter(row -> Year(row[:date]) != Year(2019), precipitation_daily_sum)\n",
    "precipitation_daily_sum_pred  = filter(row -> Year(row[:date]) == Year(2019), precipitation_daily_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "send to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter!(row -> row.date in surverse_df[!, :date], precipitation_daily_sum_train)\n",
    "filter!(row -> row.date in precipitation_daily_sum_train[!, :date], surverse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"data/new_datasets/precipitation_daily_sum/x_train.csv\", precipitation_daily_sum_train)\n",
    "CSV.write(\"data/new_datasets/precipitation_daily_sum/x_pred.csv\", precipitation_daily_sum_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Maximum as 5 Explicative Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction du taux horaire journalier maximum des précipitations pour chacune des stations météorologiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precipitation_daily_max = by(precipitation_df, :date,  McTavish = :McTavish=>maximum, Bellevue = :Bellevue=>maximum, \n",
    "   Assomption = :Assomption=>maximum, Trudeau = :Trudeau=>maximum, StHubert = :StHubert=>maximum)\n",
    "first(precipitation_daily_max,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter out 2019 year for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_daily_max_train = filter(row -> Year(row[:date]) != Year(2019), precipitation_daily_max)\n",
    "precipitation_daily_max_pred  = filter(row -> Year(row[:date]) == Year(2019), precipitation_daily_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "send to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ensure that dates fit for y and x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter!(row -> row.date in surverse_df[!, :date], precipitation_daily_max_train)\n",
    "filter!(row -> row.date in precipitation_daily_max_train[!, :date], surverse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"./data/new_datasets/precipitation_daily_max/x_train.csv\", precipitation_daily_max_train)\n",
    "CSV.write(\"./data/new_datasets/precipitation_daily_max/x_pred.csv\", precipitation_daily_max_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"./data/new_datasets/surverse_list.csv\", surverse_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Somme maximale sur une division de journée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filledPrec  = CSV.read(\"data/new_datasets/precipitation_filed_mean_per_hour.csv\", missingstring=\"-99999\")\n",
    "\n",
    "first(filledPrec,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(filledPrec,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function dayPrecipitationSplit(window,precipitations)\n",
    "    n = size(precipitations,1)\n",
    "    newDf = DataFrame(McTavish = Int64[], Bellevue = Int64[], Assomption = Int64[], Trudeau  = Int64[],\n",
    "                        StHubert = Int64[], heureDebut = Int64[], heureFin = Int64[], date = Date[])\n",
    "\n",
    "    hourgroups = 24/window\n",
    "    for day in groupby(filledPrec, :date)\n",
    "        start = 1\n",
    "        finish = window\n",
    "        date = day[1,:date]\n",
    "        if(size(day, 1)==24)\n",
    "            for i=1:hourgroups\n",
    "                mcTavish, Bellevue, Assomption, Trudeau, StHubert, heureDebut, heureFin = 0,0,0,0,0,0,0\n",
    "                for j=start:finish\n",
    "                    mcTavish += day[j,:McTavish]\n",
    "                    Assomption += day[j,:Assomption]\n",
    "                    Bellevue += day[j,:Bellevue]\n",
    "                    Trudeau += day[j,:Trudeau]\n",
    "                    StHubert += day[j,:StHubert]\n",
    "                    if j == start\n",
    "                        heureDebut = day[j,:heure]\n",
    "                    elseif j == finish\n",
    "                        heureFin = day[j,:heure]\n",
    "                    end\n",
    "                end\n",
    "                start += window\n",
    "                finish += window\n",
    "                push!(newDf,[mcTavish,Bellevue,Assomption,Trudeau,StHubert,heureDebut,heureFin,date])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return newDf\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourSplit2 = dayPrecipitationSplit(2,filledPrec)\n",
    "hourSplit3 = dayPrecipitationSplit(3,filledPrec)\n",
    "hourSplit4 = dayPrecipitationSplit(4,filledPrec)\n",
    "hourSplit6 = dayPrecipitationSplit(6,filledPrec)\n",
    "hourSplit8 = dayPrecipitationSplit(8,filledPrec)\n",
    "hourSplit12 = dayPrecipitationSplit(12,filledPrec)\n",
    "first(hourSplit12,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function maxPrecByDay(Prec)\n",
    "    n = size(Prec,1)\n",
    "    newDf = DataFrame(McTavish = Int64[], Bellevue = Int64[], Assomption = Int64[], Trudeau  = Int64[],\n",
    "                        StHubert = Int64[], date = Date[])\n",
    "\n",
    "    for day in groupby(Prec, :date)\n",
    "        mcTavish = maximum(day[:,:McTavish])\n",
    "        Assomption = maximum(day[:,:Assomption])\n",
    "        Bellevue = maximum(day[:,:Bellevue])\n",
    "        Trudeau = maximum(day[:,:Trudeau])\n",
    "        StHubert = maximum(day[:,:StHubert])\n",
    "        date = day[1,:date]\n",
    "        push!(newDf,[mcTavish,Bellevue,Assomption,Trudeau,StHubert,date])\n",
    "    end\n",
    "    return newDf\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSum2hours = maxPrecByDay(hourSplit2)\n",
    "maxSum3hours = maxPrecByDay(hourSplit3)\n",
    "maxSum4hours = maxPrecByDay(hourSplit4)\n",
    "maxSum6hours = maxPrecByDay(hourSplit6)\n",
    "maxSum8hours = maxPrecByDay(hourSplit8)\n",
    "maxSum12hours = maxPrecByDay(hourSplit12)\n",
    "first(maxSum12hours,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy2hours.csv\",maxSum2hours)\n",
    "CSV.write(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy3hours.csv\",maxSum3hours)\n",
    "CSV.write(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy4hours.csv\",maxSum4hours)\n",
    "CSV.write(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy6hours.csv\",maxSum6hours)\n",
    "CSV.write(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy8hours.csv\",maxSum8hours)\n",
    "CSV.write(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy12hours.csv\",maxSum12hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Somme de la journee + les deux dernieres heures du jour d'avant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filledPrec  = CSV.read(\"data/new_datasets/precipitation_filed_mean_per_hour.csv\", missingstring=\"-99999\")\n",
    "\n",
    "first(filledPrec,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i=1:size(precipitation_daily_sum, 1)\n",
    "    ind = findfirst(filledPrec[:,:date] .== precipitation_daily_sum[i,:date])\n",
    "    for h=1:2\n",
    "        for key in names(precipitation_daily_sum[:, Not(:date)])\n",
    "            if ind-h > 0\n",
    "                precipitation_daily_sum[i, key] += filledPrec[ind-h, key]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first(precipitation_daily_sum, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"./data/new_datasets/sum_day_last_2.csv\",precipitation_daily_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
