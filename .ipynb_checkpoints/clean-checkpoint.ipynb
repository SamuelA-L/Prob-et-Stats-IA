{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MTH3302 : Méthodes probabilistes et statistiques pour l'I.A.\n",
    "\n",
    "Jonathan Jalbert<br/>\n",
    "Professeur adjoint au Département de mathématiques et de génie industriel<br/>\n",
    "Polytechnique Montréal<br/>\n",
    "\n",
    "Le projet a été développé à l'aide de Alice Breton, étudiante à la maîtrise en génie informatique. Elle a suivi le cours lors de la session Hiver 2019.\n",
    "\n",
    "\n",
    "\n",
    "# Projet : Débordement d'égouts\n",
    "\n",
    "La description du projet est disponible à l'adresse suivante :\n",
    "https://www.kaggle.com/t/a238b752c33a41d9803c2cdde6bfc929\n",
    "\n",
    "Ce calepin Jupyter de base permet de charger et de nettoyer les données fournies. La dernière section détaille la génération du fichier des prédictions afin de le soumettre sur Kaggle dans le bon format.\n",
    "\n",
    "Dans un premier temps, vous devrez récupérer l'archive *data.zip* sur Moodle. Ce dossier contient les fichiers suivants :\n",
    "- surverses.csv\n",
    "- precipitation.csv\n",
    "- ouvrages-surverses.csv\n",
    "- test.csv\n",
    "\n",
    "Veuillez le décompresser dans le répertoire de ce calepin.\n",
    "\n",
    "Le fichier *surverse.csv* répertorie s'il y a surverse (1) ou non (0) au cours de la journée pour les 170 ouvrages de débordement de 2013 à 2018 pour les mois de mai à octobre (inclusivement). Des renseignements additionnels sur les données sont disponibles à l'adresse suivante :\n",
    "\n",
    "http://donnees.ville.montreal.qc.ca/dataset/debordement\n",
    "\n",
    "\n",
    "Le fichier *precipitation.csv* contient les précipitations horaires en dixième de *mm* enregistrées à 5 stations pluviométriques de 2013 à 2019 :\n",
    "- McTavish (7024745)\n",
    "- Ste-Anne-de-Bellevue (702FHL8)\n",
    "- Montreal/Pierre Elliott Trudeau Intl (702S006)\n",
    "- Montreal/St-Hubert (7027329)\n",
    "- L’Assomption (7014160)\n",
    "\n",
    "Plus d'informations sur les précipitations sont disponibles à l'adresse suivante :\n",
    "\n",
    "https://climat.meteo.gc.ca/climate_data/hourly_data_f.html?hlyRange=2008-01-08%7C2019-11-12&dlyRange=2002-12-23%7C2019-11-12&mlyRange=%7C&StationID=30165&Prov=QC&urlExtension=_f.html&searchType=stnName&optLimit=yearRange&StartYear=1840&EndYear=2019&selRowPerPage=25&Line=17&searchMethod=contains&Month=11&Day=12&txtStationName=montreal&timeframe=1&Year=2019\n",
    "\n",
    "Le fichier *ouvrages-surverses.csv* contient différentes caractéristiques des ouvrages de débordement. \n",
    "\n",
    "http://donnees.ville.montreal.qc.ca/dataset/ouvrage-surverse\n",
    "\n",
    "Le fichier *test.csv* contient les ouvrages et les jours pour lesquels vous devez prédire s'il y a eu surverse (true) ou non (false). Notez que l'on s'intéresse ici à 5 ouvrages de débordement localisés tout autour de l'Ile de Montréal :\n",
    "- 3260-01D dans Rivière-des-Prairies \n",
    "- 3350-07D dans Ahunstic \n",
    "- 4240-01D dans Pointe-aux-Trembles \n",
    "- 4350-01D dans le Vieux-Montréal \n",
    "- 4380-01D dans Verdun\n",
    "\n",
    "#### Remarque\n",
    "\n",
    "Dans le projet, on ne s'intéresse qu'aux surverses occasionnées par les précipitations. On ignore les surverses occasionnées par \n",
    "- fonte de neige (F)\n",
    "- travaux planifiés et entretien (TPL)\n",
    "- urgence (U)\n",
    "- autre (AUT)\n",
    "\n",
    "On suppose que lorsqu'il n'y a pas de raison pour la surverse, il s'agit d'une surverse causée par les précipitations. Puisque Nous nous intéresserons uniquement aux surverses occasionnées par les précipitations liquides, nous ne considérons que les mois de mai à octobre inclusivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics, Dates, Gadfly, LinearAlgebra, Distributions, Random, ScikitLearn, GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions gobales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    splitdataframe(df::DataFrame, p::Real)\n",
    "\n",
    "Partitionne en un ensemble d'entraînement et un ensemble de validation un DataFrame.\n",
    "\n",
    "### Arguments\n",
    "- `df::DataFrame` : Un DataFrame\n",
    "- `p::Real` : La proportion (entre 0 et 1) de données dans l'ensemble d'entraînement.\n",
    "\n",
    "### Détails\n",
    "\n",
    "La fonction renvoie deux DataFrames, un pour l'ensemble d'entraînement et l'autre pour l'ensemble de validation.\n",
    "\n",
    "### Exemple\n",
    "\n",
    "\\```\n",
    " julia> splitdataframe(df, p.7)\n",
    "\\```\n",
    "\n",
    "\"\"\"\n",
    "function splitdataframe(df::DataFrame, p::Real)\n",
    "   @assert 0 <= p <= 1 \n",
    "    \n",
    "    n = size(df,1)\n",
    "    \n",
    "    ind = shuffle(1:n)\n",
    "    \n",
    "    threshold = Int64(round(n*p))\n",
    "    \n",
    "    indTrain = sort(ind[1:threshold])\n",
    "    \n",
    "    indTest = setdiff(1:n,indTrain)\n",
    "    \n",
    "    dfTrain = df[indTrain,:]\n",
    "    dfTest = df[indTest,:]\n",
    "    \n",
    "    return dfTrain, dfTest\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données et nettoyage préliminaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des surverses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CSV.read(\"data/surverses.csv\",missingstring=\"-99999\")\n",
    "first(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données sur les surverses\n",
    "\n",
    "#### Extraction des surverses pour les mois de mai à octobre inclusivement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter(row -> month(row.DATE) > 4, data) \n",
    "data = filter(row -> month(row.DATE) < 11, data) \n",
    "first(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remplacement des valeurs *missing* dans la colonne :RAISON par \"Inconnue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raison = coalesce.(data[:,:RAISON],\"Inconnue\")\n",
    "data[!,:RAISON] = raison\n",
    "first(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exlusion des surverses coccasionnées par d'autres facteurs que les précipitations liquides\n",
    "\n",
    "Ces facteurs correspondent à : \n",
    "- la fonte de neige (F), \n",
    "- les travaux planifiés et entretien (TPL)\n",
    "- urgence (U)\n",
    "- autre (AUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter(row -> row.RAISON ∈ [\"P\",\"Inconnue\",\"TS\"], data) \n",
    "select!(data, [:NO_OUVRAGE, :DATE, :SURVERSE])\n",
    "first(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusion des lignes où :SURVERSE est manquante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surverse_df = dropmissing(data, disallowmissing=true)\n",
    "first(surverse_df,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"surverse_df.csv\", surverse_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des précipitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CSV.read(\"data/precipitations.csv\",missingstring=\"-99999\")\n",
    "rename!(data, Symbol(\"St-Hubert\")=>:StHubert)\n",
    "first(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données sur les précipitations\n",
    "\n",
    "#### Extraction des précipitations des mois de mai à octobre inclusivement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter(row -> month(row.date) > 4, data) \n",
    "data = filter(row -> month(row.date) < 11, data) \n",
    "first(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Remplissage des données manquantes\n",
    "Nous allons tenter de remplir les données manquantes par des moyennes de précipitations lorsque les données sont inconnues pour 2 stations ou plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction permettant de compter le nombre de valeurs du type \"missing\" dans une ligne\n",
    "function countMissing(line)\n",
    "    count=0\n",
    "    ind = 0\n",
    "    for i=1:length(line)\n",
    "        if (ismissing(line[i]))\n",
    "            count += 1\n",
    "            ind = i\n",
    "        end\n",
    "    end\n",
    "    return count, ind\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction permettant de calculer le moyenne des valeurs d'une ligne peu importe le nombre de \"missing\"\n",
    "function meanLine(line)\n",
    "    values = []\n",
    "    for i=1:length(line)\n",
    "        if (!ismissing(line[i]))\n",
    "            append!(values,line[i])\n",
    "        end\n",
    "    end\n",
    "    return mean(values)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction permettant de remplacer toutes les valeurs \"missing\" d'une ligne avec la valeur donnée en parametre\n",
    "function replaceMissing(line,value)\n",
    "    n = length(line)\n",
    "    for i=1:n\n",
    "        if (ismissing(line[i]))\n",
    "            line[i] = value\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire le remplissage des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = length(data[:,:date])\n",
    "prec = data[:,Not(:date)][:,Not(:heure)]\n",
    "nbMissing = 0\n",
    "\n",
    "for row in eachrow(prec)\n",
    "    nbMissing, ind = countMissing(row)\n",
    "    if(nbMissing == 1)\n",
    "        row[ind] = round((convert(Vector{Float64},row[Not(ind)])'*betas[:, :β][ind]))\n",
    "    end\n",
    "    # remplacer les lignes qui ont de 2 a 4 missing\n",
    "    if(nbMissing<5 && nbMissing>1)\n",
    "        replaceMissing(row,round(meanLine(row)))\n",
    "    end\n",
    "end\n",
    "first(prec,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec.heure = data[:,:heure]\n",
    "prec.date = data[:,:date]\n",
    "first(prec,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_values = prec[: ,[:date, :heure, :McTavish, :Bellevue, :Assomption, :Trudeau, :StHubert]]\n",
    "CSV.write(\"clean_values.csv\",clean_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul des max et des sums par jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_sum = by(prec, :date,  McTavish = :McTavish=>sum, Bellevue = :Bellevue=>sum, \n",
    "   Assomption = :Assomption=>sum, Trudeau = :Trudeau=>sum, StHubert = :StHubert=>sum)\n",
    "first(pcp_sum ,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction du taux horaire journalier maximum des précipitations pour chacune des stations météorologiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_max = by(prec, :date,  McTavish = :McTavish=>maximum, Bellevue = :Bellevue=>maximum, \n",
    "   Assomption = :Assomption=>maximum, Trudeau = :Trudeau=>maximum, StHubert = :StHubert=>maximum)\n",
    "first(pcp_max,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction pour calculer la precision d'un modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function precision(model,test)\n",
    "    \n",
    "    reference = test[!,:Y]\n",
    "    predictions = GLM.predict(model,test)\n",
    "    n = length(predictions)\n",
    "    roundedPredictions = round.(predictions)\n",
    "    goodP = 0\n",
    "    badP = 0\n",
    "    for i=1:n\n",
    "        if roundedPredictions[i] == reference[i]\n",
    "            goodP +=1\n",
    "        else\n",
    "            badP +=1\n",
    "        end\n",
    "    end\n",
    "    println(\"succes = $goodP\")\n",
    "    println(\"failiure = $badP\")\n",
    "    percentage = goodP/n\n",
    "    println(\"accuracy = $percentage\")\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction pour créer un modèle logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function modeleLogistique(ouvrage,pcp_max,pcp_sum,surverse_df)\n",
    "    \n",
    "    \n",
    "    df = filter(row -> row.NO_OUVRAGE == ouvrage, surverse_df)\n",
    "\n",
    "    x₁ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var somme journalière--McTavish\n",
    "    x₂ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var max journalier-----McTavish\n",
    "    x₃ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var somme journalière--Bellevue\n",
    "    x₄ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var max journalier-----Bellevue\n",
    "    x₅ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var somme journalière--\n",
    "    x₆ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var max journalier-----\n",
    "    x₇ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var somme journalière\n",
    "    x₈ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var max journalier\n",
    "    x₉ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var somme journalière\n",
    "    x₁₀ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var max journalier\n",
    "\n",
    "\n",
    "\n",
    "    for i=1:size(df,1)\n",
    "\n",
    "        ind = findfirst(pcp_sum[:,:date] .== df[i,:DATE])\n",
    "        x₁[i] = pcp_sum[ind,:McTavish]\n",
    "        ind = findfirst(pcp_max[:,:date] .== df[i,:DATE])\n",
    "        x₂[i] = pcp_max[ind,:McTavish]\n",
    "\n",
    "        ind = findfirst(pcp_sum[:,:date] .== df[i,:DATE])\n",
    "        x₃[i] = pcp_sum[ind,:Bellevue]\n",
    "        ind = findfirst(pcp_max[:,:date] .== df[i,:DATE])\n",
    "        x₄[i] = pcp_max[ind,:Bellevue]\n",
    "\n",
    "        ind = findfirst(pcp_sum[:,:date] .== df[i,:DATE])\n",
    "        x₅[i] = pcp_sum[ind,:Assomption]\n",
    "        ind = findfirst(pcp_max[:,:date] .== df[i,:DATE])\n",
    "        x₆[i] = pcp_max[ind,:Assomption]\n",
    "\n",
    "        ind = findfirst(pcp_sum[:,:date] .== df[i,:DATE])\n",
    "        x₇[i] = pcp_sum[ind,:Trudeau]\n",
    "        ind = findfirst(pcp_max[:,:date] .== df[i,:DATE])\n",
    "        x₈[i] = pcp_max[ind,:Trudeau]\n",
    "\n",
    "        ind = findfirst(pcp_sum[:,:date] .== df[i,:DATE])\n",
    "        x₉[i] = pcp_sum[ind,:StHubert]\n",
    "        ind = findfirst(pcp_max[:,:date] .== df[i,:DATE])\n",
    "        x₁₀[i] = pcp_max[ind,:StHubert]\n",
    "\n",
    "    end\n",
    "\n",
    "    df[!,:sumMcTavish] = x₁\n",
    "    df[!,:maxMcTavish] = x₂\n",
    "    df[!,:sumBellevue] = x₃\n",
    "    df[!,:maxBellevue] = x₄\n",
    "    df[!,:sumAssomption] = x₅\n",
    "    df[!,:maxAssomption] = x₆\n",
    "    df[!,:sumTrudeau] = x₇\n",
    "    df[!,:maxTrudeau] = x₈\n",
    "    df[!,:sumStHubert] = x₉\n",
    "    df[!,:maxStHubert] = x₁₀\n",
    "    \n",
    "    dataframe = DataFrame(x₁=x₁,x₂=x₂,x₃=x₃,x₄=x₄,x₅=x₅,x₆=x₆,x₇=x₇,x₈=x₈,x₉=x₉,x₁₀=x₁₀,Y=df.SURVERSE)\n",
    "    dropmissing!(dataframe)\n",
    "    println(length(dataframe[:,:Y]))\n",
    "    train,test = splitdataframe(dataframe,0.80)\n",
    "    logicModel = glm(@formula(Y ~ x₁+x₂+x₃+x₅+x₆+x₇+x₈+x₉+x₁₀), train,  Bernoulli(), LogitLink())\n",
    "    precision(logicModel,test)\n",
    "    return logicModel\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mvieuxmtl = modeleLogistique(\"4350-01D\",pcp_max,pcp_sum,surverse_df)\n",
    "Mrdesp = modeleLogistique(\"3260-01D\",pcp_max,pcp_sum,surverse_df)\n",
    "Mahunstic = modeleLogistique(\"3350-07D\",pcp_max,pcp_sum,surverse_df)\n",
    "Mpauxt = modeleLogistique(\"4240-01D\",pcp_max,pcp_sum,surverse_df)\n",
    "Mverdun = modeleLogistique(\"4380-01D\",pcp_max,pcp_sum,surverse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store models in Dict for easier acces DID NOT WORK\n",
    "#modelsDict = Dict{\"4350-01D\" => Mvieuxmtl, \"3260-01D\"=>Mrdesp, \"3350-07D\"=>Mahunstic, \"4240-01D\"=>Mpauxt, \"4380-01D\"=>Mverdun}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inclusion dans un dataframe de ces deux variables explicatives potentielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouvrage = \"4350-01D\"\n",
    "\n",
    "# df = filter(row -> row.NO_OUVRAGE == ouvrage, surverse_df)\n",
    "\n",
    "# x₁ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var somme journalière--McTavish\n",
    "# x₂ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var max journalier-----McTavish\n",
    "# x₃ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var somme journalière--Bellevue\n",
    "# x₄ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var max journalier-----Bellevue\n",
    "# x₅ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var somme journalière--\n",
    "# x₆ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var max journalier-----\n",
    "# x₇ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var somme journalière\n",
    "# x₈ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var max journalier\n",
    "# x₉ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var somme journalière\n",
    "# x₁₀ = Array{Union{Missing, Int64}}(missing, size(df,1)) # var max journalier\n",
    "\n",
    "\n",
    "\n",
    "# for i=1:size(df,1)\n",
    "    \n",
    "#     ind = findfirst(pcp_sum[:,:date] .== df[i,:DATE])\n",
    "#     x₁[i] = pcp_sum[ind,:McTavish]\n",
    "#     ind = findfirst(pcp_max[:,:date] .== df[i,:DATE])\n",
    "#     x₂[i] = pcp_max[ind,:McTavish]\n",
    "    \n",
    "#     ind = findfirst(pcp_sum[:,:date] .== df[i,:DATE])\n",
    "#     x₃[i] = pcp_sum[ind,:Bellevue]\n",
    "#     ind = findfirst(pcp_max[:,:date] .== df[i,:DATE])\n",
    "#     x₄[i] = pcp_max[ind,:Bellevue]\n",
    "    \n",
    "#     ind = findfirst(pcp_sum[:,:date] .== df[i,:DATE])\n",
    "#     x₅[i] = pcp_sum[ind,:Assomption]\n",
    "#     ind = findfirst(pcp_max[:,:date] .== df[i,:DATE])\n",
    "#     x₆[i] = pcp_max[ind,:Assomption]\n",
    "    \n",
    "#     ind = findfirst(pcp_sum[:,:date] .== df[i,:DATE])\n",
    "#     x₇[i] = pcp_sum[ind,:Trudeau]\n",
    "#     ind = findfirst(pcp_max[:,:date] .== df[i,:DATE])\n",
    "#     x₈[i] = pcp_max[ind,:Trudeau]\n",
    "    \n",
    "#     ind = findfirst(pcp_sum[:,:date] .== df[i,:DATE])\n",
    "#     x₉[i] = pcp_sum[ind,:StHubert]\n",
    "#     ind = findfirst(pcp_max[:,:date] .== df[i,:DATE])\n",
    "#     x₁₀[i] = pcp_max[ind,:StHubert]\n",
    "    \n",
    "# end\n",
    "\n",
    "# df[!,:sumMcTavish] = x₁\n",
    "# df[!,:maxMcTavish] = x₂\n",
    "# df[!,:sumBellevue] = x₃\n",
    "# df[!,:maxBellevue] = x₄\n",
    "# df[!,:sumAssomption] = x₅\n",
    "# df[!,:maxAssomption] = x₆\n",
    "# df[!,:sumTrudeau] = x₇\n",
    "# df[!,:maxTrudeau] = x₈\n",
    "# df[!,:sumStHubert] = x₉\n",
    "# df[!,:maxStHubert] = x₁₀\n",
    "\n",
    "\n",
    "# #dropmissing!(df, [:SUM, :MAX],disallowmissing=true)\n",
    "# first(df,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traçage des distribution de la somme des précipitations en fonction des surverses ou non\n",
    "\n",
    "On remarque que les deux distributions sont très différentes. Ceci suggère que la somme des précipitations à la station McTavish a un effet sur les surverses au Bota-Bota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(df, x=:SURVERSE, y=:SUM, Geom.boxplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traçage des distribution de la somme des précipitations en fonction des surverses ou non\n",
    "\n",
    "On remarque que les deux distributions sont très différentes. Ceci suggère que le maximum journalier des précipitations à la station McTavish a un effet sur les surverses au Bota-Bota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(df, x=:SURVERSE, y=:MAX, Geom.boxplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place des valeurs cherchees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CSV.read(\"data/test.csv\")\n",
    "df = test[:, [:NO_OUVRAGE, :DATE]]\n",
    "rename!(df, :DATE=>:date)\n",
    "for name in names(pcp_max[:, Not(:date)])\n",
    "    name2 = name\n",
    "    rename!(pcp_max, name=>Symbol(\"max\" * String(name)))\n",
    "    rename!(pcp_sum, name2=>Symbol(\"sum\" * String(name2)))\n",
    "end\n",
    "df = join(df, pcp_max, on=:date, kind= :inner)\n",
    "df = join(df, pcp_sum, on=:date, kind= :inner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = DataFrame(ouvrage=df.NO_OUVRAGE, x₁=df.sumMcTavish,x₂=df.maxMcTavish,x₃=df.sumBellevue,x₄=df.maxBellevue,x₅=df.sumAssomption,x₆=df.maxAssomption,x₇=df.sumTrudeau,x₈=df.maxTrudeau,x₉=df.sumStHubert,x₁₀=df.maxStHubert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for row in eachrow(dataframe)\n",
    "    if row.ouvrage == \"4350-01D\" \n",
    "        predictions = vcat(predictions, GLM.predict(Mvieuxmtl, DataFrame(row[Not(:ouvrage)])))\n",
    "    end\n",
    "    if row.ouvrage == \"3260-01D\"\n",
    "        predictions = vcat(predictions, GLM.predict(Mrdesp, DataFrame(row[Not(:ouvrage)])))\n",
    "    end\n",
    "    if row.ouvrage == \"3350-07D\"\n",
    "        predictions = vcat(predictions, GLM.predict(Mahunstic, DataFrame(row[Not(:ouvrage)])))\n",
    "    end\n",
    "    if row.ouvrage == \"4240-01D\"\n",
    "        predictions = vcat(predictions, GLM.predict(Mpauxt, DataFrame(row[Not(:ouvrage)])))\n",
    "    end\n",
    "    if row.ouvrage == \"4380-01D\"\n",
    "        predictions = vcat(predictions, GLM.predict(Mverdun, DataFrame(row[Not(:ouvrage)])))\n",
    "    end\n",
    "end\n",
    "predictions = round.(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du fichier de prédictions pour soumettre sur Kaggle\n",
    "\n",
    "Dans ce cas-ci, nous prédirons une surverse avec une prediction logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pour chacune des lignes du fichier test, comportant un ouvrage et une date, une prédiction est requise.\n",
    "# Dans ce cas-ci, utilisons une prédiction les plus naîve. \n",
    "# On prédit avec une chance sur deux qu'il y ait surverse, sans utiliser de variables explicatives\n",
    "#n = size(test,1)\n",
    "#surverse = rand(n) .> .5\n",
    "\n",
    "\n",
    "# Création du fichier sampleSubmission.csv pour soumettre sur Kaggle\n",
    "ID = test[:,:NO_OUVRAGE].*\"_\".*string.(test[:,:DATE])\n",
    "sampleSubmission = DataFrame(ID = ID, Surverse=predictions)\n",
    "CSV.write(\"sampleSubmission.csv\",sampleSubmission)\n",
    "\n",
    "# Vous pouvez par la suite déposer le fichier sampleSubmission.csv sur Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
