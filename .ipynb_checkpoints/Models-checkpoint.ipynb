{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT & SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics, Dates, Gadfly, LinearAlgebra, Distributions, Random, ScikitLearn, GLM, Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction générique pour former un dataframe contenant les variables explicatives\n",
    "# Array of data contient les dataframes des va explicatives\n",
    "#list_of_va contient le type des données i.e [\"sum\" \"max\" ...]\n",
    "# surverse contient les données de surverse\n",
    "function createDataEx(array_of_data, list_of_va, dates)\n",
    "    df = DataFrame(date = dates)\n",
    "    \n",
    "    for va in 1:length(list_of_va)\n",
    "        array = array_of_data[va]\n",
    "        McTavish = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        Bellevue = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        Assomption = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        Trudeau = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        StHubert = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        \n",
    "        for i=1:size(df,1)\n",
    "            ind = findfirst(array[:,:date] .== df[i,:date])\n",
    "            McTavish[i] = array[ind,:McTavish]\n",
    "            Bellevue[i] = array[ind,:Bellevue]\n",
    "            Assomption[i] = array[ind,:Assomption]\n",
    "            Trudeau[i] = array[ind,:Trudeau]\n",
    "            StHubert[i] = array[ind,:StHubert]\n",
    "        end\n",
    "        \n",
    "        df[!,Symbol(list_of_va[va] * \"McTavish\")] = McTavish\n",
    "        df[!,Symbol(list_of_va[va] * \"Bellevue\")] = Bellevue   \n",
    "        df[!,Symbol(list_of_va[va] * \"Assomption\")] = Assomption   \n",
    "        df[!,Symbol(list_of_va[va] * \"Trudeau\")] = Trudeau   \n",
    "        df[!,Symbol(list_of_va[va] * \"StHubert\")] = StHubert\n",
    "    end\n",
    "    \n",
    "    return df\n",
    "end\n",
    "\n",
    "#fonction pour recuperer seulement les donnees d'un ouvrage\n",
    "#fonction pour filtrer un dataframe selon l<ouvrage\n",
    "function getOuvrage(data, ouvrage)\n",
    "    return filter(row -> row.NO_OUVRAGE == ouvrage, data)\n",
    "end\n",
    "\n",
    "#fonction pour recuperer seulement les donnees contenu a bonnes dates\n",
    "function getDataFromDates(df, dates)\n",
    "    return filter(row -> row.date in dates, df)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    splitdataframe(df::DataFrame, p::Real)\n",
    "\n",
    "Partitionne en un ensemble d'entraînement et un ensemble de validation un DataFrame.\n",
    "\n",
    "### Arguments\n",
    "- `df::DataFrame` : Un DataFrame\n",
    "- `p::Real` : La proportion (entre 0 et 1) de données dans l'ensemble d'entraînement.\n",
    "\n",
    "### Détails\n",
    "\n",
    "La fonction renvoie deux DataFrames, un pour l'ensemble d'entraînement et l'autre pour l'ensemble de validation.\n",
    "\n",
    "### Exemple\n",
    "\n",
    "\\```\n",
    " julia> splitdataframe(df, p.7)\n",
    "\\```\n",
    "\n",
    "\"\"\"\n",
    "function splitdataframe(df::DataFrame, p::Real)\n",
    "   @assert 0 <= p <= 1 \n",
    "    \n",
    "    n = size(df,1)\n",
    "    \n",
    "    ind = shuffle(1:n)\n",
    "    \n",
    "    threshold = Int64(round(n*p))\n",
    "    \n",
    "    indTrain = sort(ind[1:threshold])\n",
    "    \n",
    "    indTest = setdiff(1:n,indTrain)\n",
    "    \n",
    "    dfTrain = df[indTrain,:]\n",
    "    dfTest = df[indTest,:]\n",
    "    \n",
    "    return dfTrain, dfTest\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction pour la conversion d'un dataframe en matrix/array\n",
    "On ignore les dates et les ouvrages si necessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function toArray(df::DataFrame)\n",
    "    \n",
    "    if :NO_OUVRAGE in names(df)\n",
    "        return convert(Matrix, df[:, Not([:date, :NO_OUVRAGE])])\n",
    "    elseif :date in names(df)\n",
    "        return convert(Matrix, df[:, Not(:date)])\n",
    "    else\n",
    "        return convert(Matrix, df)\n",
    "    end\n",
    "end\n",
    "function toArray(df::DataFrameRow)\n",
    "    \n",
    "    if :NO_OUVRAGE in names(df)\n",
    "        return convert(Vector, df[Not([:date, :NO_OUVRAGE])])\n",
    "    elseif :date in names(df)\n",
    "        return convert(Vector, df[Not(:date)])\n",
    "    else\n",
    "        return convert(Vector, df)\n",
    "    end\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture des fichiers des variables explicatives et des surverses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_max = CSV.read(\"./data/new_datasets/precipitation_daily_max/x_train.csv\");\n",
    "x_pred_max = CSV.read(\"./data/new_datasets/precipitation_daily_max/x_pred.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sum = CSV.read(\"./data/new_datasets/precipitation_daily_sum/x_train.csv\");\n",
    "x_pred_sum = CSV.read(\"./data/new_datasets/precipitation_daily_sum/x_pred.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum_last_2 = CSV.read(\"./data/new_datasets/sum_day_last_2.csv\")\n",
    "x_train_sum_last_2 = filter(row -> Year(row[:date]) != Year(2019), x_sum_last_2)\n",
    "x__pred_sum_last_2  = filter(row -> Year(row[:date]) == Year(2019), x_sum_last_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_2 = CSV.read(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy2hours.csv\")\n",
    "x_train_max_2 = filter(row -> Year(row[:date]) != Year(2019), x_max_2)\n",
    "x_pred_max_2  = filter(row -> Year(row[:date]) == Year(2019), x_max_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_3 = CSV.read(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy3hours.csv\")\n",
    "x_train_max_3 = filter(row -> Year(row[:date]) != Year(2019), x_max_3)\n",
    "x_pred_max_3  = filter(row -> Year(row[:date]) == Year(2019), x_max_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_4 = CSV.read(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy4hours.csv\")\n",
    "x_train_max_4 = filter(row -> Year(row[:date]) != Year(2019), x_max_4)\n",
    "x_pred_max_4  = filter(row -> Year(row[:date]) == Year(2019), x_max_4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_6 = CSV.read(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy6hours.csv\")\n",
    "x_train_max_6 = filter(row -> Year(row[:date]) != Year(2019), x_max_6)\n",
    "x_pred_max_6  = filter(row -> Year(row[:date]) == Year(2019), x_max_6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_8 = CSV.read(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy8hours.csv\")\n",
    "x_train_max_8 = filter(row -> Year(row[:date]) != Year(2019), x_max_8)\n",
    "x_pred_max_8  = filter(row -> Year(row[:date]) == Year(2019), x_max_8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_12 = CSV.read(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy12hours.csv\")\n",
    "x_train_max_12 = filter(row -> Year(row[:date]) != Year(2019), x_max_12)\n",
    "x_pred_max_12  = filter(row -> Year(row[:date]) == Year(2019), x_max_12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding which variables to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = CSV.read(\"./data/new_datasets/surverse_list.csv\");\n",
    "x_train = createDataEx([x_train_max, x_train_sum, x_train_max_2, x_train_max_3, x_train_max_4, x_train_max_2], [\"max\", \"sum\", \"max2\", \"max3\", \"max4\"], x_train_max_2[:, :date])\n",
    "x_pred = createDataEx([x_pred_max, x_pred_sum, x_pred_max_2, x_pred_max_3, x_pred_max_4, x_pred_max_2], [\"max\", \"sum\", \"max2\", \"max3\", \"max4\"], x_pred_max_2[:, :date])\n",
    "dropmissing!(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouvrages qui nous interesse\n",
    "ouvrages = [\"3260-01D\", \"3350-07D\", \"4240-01D\", \"4350-01D\", \"4380-01D\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histogram(convert(Matrix, x_train[:, Not(:date)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that most data is skewed towards 0cm with some extreme cases over 2000cm (outliers)\n",
    "\n",
    "we need to preprocess the data with a function to reduce this gap\n",
    "\n",
    "$ f(x) = ln(x+1) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = DataFrame([log.(col.+1) for col = eachcol(x_train[:, Not(:date)])])\n",
    "display(histogram(convert(Matrix, x_values)))\n",
    "display(describe(x_values, :mean, :std, :min, :q25, :median, :q75, :max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_log = log.(x_train[:, Not(:date)] .+ 1)\n",
    "x_pred_log = log.(x_pred[:, Not(:date)] .+ 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTICOLINEARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CUTOFF = 7\n",
    "\n",
    "@sk_import decomposition : PCA\n",
    "pca = PCA()\n",
    "pca.fit(toArray(x_train))\n",
    "\n",
    "explained_variance = cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "Plots.plot(\n",
    "    1:length(pca.explained_variance_ratio_),\n",
    "    explained_variance,\n",
    "    ylims=(0, 1.0),\n",
    "    xlabel=\"Number of components\",\n",
    "    ylabel=\"Cumulative sum of explained Variance\"\n",
    ")\n",
    "display(\n",
    "    annotate!([(12.5, .4, \n",
    "        text(\"Cutoff at $(CUTOFF) variables which is $(round(explained_variance[CUTOFF], digits=5))% of the total variance\",10)\n",
    "    )])\n",
    ")\n",
    "pca = PCA(n_components=7)\n",
    "pca.fit_transform(toArray(x_train));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS\n",
    "\n",
    "### Function use to calculate the F1 score of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO needs to be generic not just for GLM\n",
    "@sk_import metrics : f1_score\n",
    "function modelF1(model,test)\n",
    "    \n",
    "    reference = test[!,:Y]\n",
    "    predictions = round.(GLM.predict(model,test))\n",
    "    \n",
    "    F1 = f1_score(reference, predictions)\n",
    "    println(\"F1 score = $F1\")\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 1st ouvrage only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function modeleLogistique(ouvrage, x_train, y_train) \n",
    "    \n",
    "    Y = getOuvrage(y_train, ouvrage)\n",
    "    X = getDataFromDates(x_train, Y.date)\n",
    "    Y = getDataFromDates(Y, X.date)\n",
    "    dataframe = DataFrame(Y=Y.SURVERSE, x₁=X.sumMcTavish,x₂=X.sumBellevue,x₃=X.sumAssomption,x₄=X.sumTrudeau,x₅=X.sumStHubert,x₆=X.maxMcTavish,x₇=X.maxBellevue,x₈=X.maxAssomption,x₉=X.maxTrudeau,x₁₀=X.maxStHubert)\n",
    "    dropmissing!(dataframe)\n",
    "    println(length(dataframe[:,:Y]))\n",
    "    train,test = splitdataframe(dataframe,0.80)\n",
    "    logicModel = glm(@formula(Y ~ x₁+x₂+x₃+x₅+x₆+x₇+x₈+x₉+x₁₀), train,  Bernoulli(), LogitLink())\n",
    "    modelF1(logicModel,test)\n",
    "    return logicModel\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelesLogit = DataFrame(NO_OUVRAGE=[], modele = [])\n",
    "for ouvrage in ouvrages\n",
    "    push!(modelesLogit, [ouvrage, modeleLogistique(ouvrage, x_train,y_train)])\n",
    "end\n",
    "modelesLogit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dont know what this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @sk_import linear_model: LogisticRegression\n",
    "# model = LogisticRegression(\n",
    "#     penalty=\"l2\",\n",
    "#     random_state=234, # chosen by random dice roll, thus truly random\n",
    "# )\n",
    "# ouvrage = getOuvrage(y_train, \"3260-01D\")\n",
    "# ScikitLearn.fit!(model, toArray(getDataFromDates(x_train, ouvrage[!, :date])), toArray(ouvrage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele de l'arbre decisif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sk_import tree: DecisionTreeClassifier\n",
    "function modeleArbreDecisif(ouvrage, x_train, y_train)\n",
    "    \n",
    "    Y = getOuvrage(y_train, ouvrage)\n",
    "    X = getDataFromDates(x_train, Y.date)\n",
    "    Y = getDataFromDates(Y, X.date)\n",
    "    println(size(X), size(Y))\n",
    "    return DecisionTreeClassifier().fit(toArray(X),toArray(Y))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelesTree = DataFrame(NO_OUVRAGE=[], modele = [])\n",
    "for ouvrage in ouvrages\n",
    "    push!(modelesTree, [ouvrage, modeleArbreDecisif(ouvrage, x_train,y_train)])\n",
    "end\n",
    "modelesTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place des valeurs cherchees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CSV.read(\"data/test.csv\")\n",
    "df = test[:, [:NO_OUVRAGE, :DATE]]\n",
    "rename!(df, :DATE=>:date)\n",
    "X = join(df, x_pred, on=:date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predictLogit(df, modeles)\n",
    "    dataframe = DataFrame(NO_OUVRAGE=df.NO_OUVRAGE, x₁=df.sumMcTavish,x₂=df.sumBellevue,x₃=df.sumAssomption,x₄=df.sumTrudeau,x₅=df.sumStHubert,x₆=df.maxMcTavish,x₇=df.maxBellevue,x₈=df.maxAssomption,x₉=df.maxTrudeau,x₁₀=df.maxStHubert)\n",
    "    predictions = []\n",
    "    for x in eachrow(dataframe)\n",
    "        modele = filter(row -> row.NO_OUVRAGE == x.NO_OUVRAGE, modeles).modele[1]\n",
    "        predictions = vcat(predictions, GLM.predict(modele, DataFrame(x[Not([:NO_OUVRAGE])])))\n",
    "    end\n",
    "    return round.(predictions)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predictTree(dataframe, modeles)\n",
    "    predictions = []\n",
    "    for x in eachrow(dataframe)\n",
    "        modele = filter(row -> row.NO_OUVRAGE == x.NO_OUVRAGE, modeles).modele[1]\n",
    "        predictions = vcat(predictions, modele.predict([toArray(x)]))\n",
    "    end\n",
    "    return round.(predictions)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsTree = predictTree(X, modelesTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsLogit = predictLogit(X, modelesLogit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nombre de diff entre Tree et Logit\n",
    "sum(abs.(predictionsTree-predictionsLogit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du fichier de prédictions pour soumettre sur Kaggle\n",
    "\n",
    "Dans ce cas-ci, nous prédirons une surverse avec une prediction logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du fichier sampleSubmission.csv pour soumettre sur Kaggle\n",
    "ID = test[:,:NO_OUVRAGE].*\"_\".*string.(test[:,:DATE])\n",
    "sampleSubmission = DataFrame(ID = ID, Surverse=predictionsTree)\n",
    "CSV.write(\"sampleSubmission.csv\",sampleSubmission)\n",
    "\n",
    "# Vous pouvez par la suite déposer le fichier sampleSubmission.csv sur Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
