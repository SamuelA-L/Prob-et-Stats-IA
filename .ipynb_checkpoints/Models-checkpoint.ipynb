{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT & SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics, Dates, Gadfly, LinearAlgebra, Distributions, Random, ScikitLearn, GLM, Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction générique pour former un dataframe contenant les variables explicatives\n",
    "# Array of data contient les dataframes des va explicatives\n",
    "#list_of_va contient le type des données i.e [\"sum\" \"max\" ...]\n",
    "# surverse contient les données de surverse\n",
    "function createDataEx(array_of_data, list_of_va, dates)\n",
    "    df = DataFrame(date = dates)\n",
    "    \n",
    "    for va in 1:length(list_of_va)\n",
    "        array = array_of_data[va]\n",
    "        McTavish = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        Bellevue = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        Assomption = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        Trudeau = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        StHubert = Array{Union{Missing, Int64}}(missing, size(df,1))\n",
    "        \n",
    "        for i=1:size(df,1)\n",
    "            ind = findfirst(array[:,:date] .== df[i,:date])\n",
    "            McTavish[i] = array[ind,:McTavish]\n",
    "            Bellevue[i] = array[ind,:Bellevue]\n",
    "            Assomption[i] = array[ind,:Assomption]\n",
    "            Trudeau[i] = array[ind,:Trudeau]\n",
    "            StHubert[i] = array[ind,:StHubert]\n",
    "        end\n",
    "        \n",
    "        df[!,Symbol(list_of_va[va] * \"McTavish\")] = McTavish\n",
    "        df[!,Symbol(list_of_va[va] * \"Bellevue\")] = Bellevue   \n",
    "        df[!,Symbol(list_of_va[va] * \"Assomption\")] = Assomption   \n",
    "        df[!,Symbol(list_of_va[va] * \"Trudeau\")] = Trudeau   \n",
    "        df[!,Symbol(list_of_va[va] * \"StHubert\")] = StHubert\n",
    "    end\n",
    "    \n",
    "    return df\n",
    "end\n",
    "\n",
    "#fonction pour recuperer seulement les donnees d'un ouvrage\n",
    "#fonction pour filtrer un dataframe selon l<ouvrage\n",
    "function getOuvrage(data, ouvrage)\n",
    "    return filter(row -> row.NO_OUVRAGE == ouvrage, data)\n",
    "end\n",
    "\n",
    "#fonction pour recuperer seulement les donnees contenu a bonnes dates\n",
    "function getDataFromDates(df, dates)\n",
    "    return filter(row -> row.date in dates, df)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    splitdataframe(df::DataFrame, p::Real)\n",
    "\n",
    "Partitionne en un ensemble d'entraînement et un ensemble de validation un DataFrame.\n",
    "\n",
    "### Arguments\n",
    "- `df::DataFrame` : Un DataFrame\n",
    "- `p::Real` : La proportion (entre 0 et 1) de données dans l'ensemble d'entraînement.\n",
    "\n",
    "### Détails\n",
    "\n",
    "La fonction renvoie deux DataFrames, un pour l'ensemble d'entraînement et l'autre pour l'ensemble de validation.\n",
    "\n",
    "### Exemple\n",
    "\n",
    "\\```\n",
    " julia> splitdataframe(df, p.7)\n",
    "\\```\n",
    "\n",
    "\"\"\"\n",
    "function splitdataframe(df::DataFrame, p::Real)\n",
    "   @assert 0 <= p <= 1 \n",
    "    \n",
    "    n = size(df,1)\n",
    "    \n",
    "    ind = shuffle(1:n)\n",
    "    \n",
    "    threshold = Int64(round(n*p))\n",
    "    \n",
    "    indTrain = sort(ind[1:threshold])\n",
    "    \n",
    "    indTest = setdiff(1:n,indTrain)\n",
    "    \n",
    "    dfTrain = df[indTrain,:]\n",
    "    dfTest = df[indTest,:]\n",
    "    \n",
    "    return dfTrain, dfTest\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction pour la conversion d'un dataframe en matrix/array\n",
    "On ignore les dates et les ouvrages si necessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function toArray(df::DataFrame)\n",
    "    \n",
    "    if :NO_OUVRAGE in names(df)\n",
    "        return convert(Matrix, df[:, Not([:date, :NO_OUVRAGE])])\n",
    "    elseif :date in names(df)\n",
    "        return convert(Matrix, df[:, Not(:date)])\n",
    "    else\n",
    "        return convert(Matrix, df)\n",
    "    end\n",
    "end\n",
    "function toArray(df::DataFrameRow)\n",
    "    \n",
    "    if :NO_OUVRAGE in names(df)\n",
    "        return convert(Vector, df[Not([:date, :NO_OUVRAGE])])\n",
    "    elseif :date in names(df)\n",
    "        return convert(Vector, df[Not(:date)])\n",
    "    else\n",
    "        return convert(Vector, df)\n",
    "    end\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture des fichiers des variables explicatives et des surverses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_max = CSV.read(\"./data/new_datasets/precipitation_daily_max/x_train.csv\");\n",
    "x_pred_max = CSV.read(\"./data/new_datasets/precipitation_daily_max/x_pred.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sum = CSV.read(\"./data/new_datasets/precipitation_daily_sum/x_train.csv\");\n",
    "x_pred_sum = CSV.read(\"./data/new_datasets/precipitation_daily_sum/x_pred.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum_last_2 = CSV.read(\"./data/new_datasets/sum_day_last_2.csv\")\n",
    "x_train_sum_last_2 = filter(row -> Year(row[:date]) != Year(2019), x_sum_last_2)\n",
    "x__pred_sum_last_2  = filter(row -> Year(row[:date]) == Year(2019), x_sum_last_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_2 = CSV.read(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy2hours.csv\")\n",
    "x_train_max_2 = filter(row -> Year(row[:date]) != Year(2019), x_max_2)\n",
    "x_pred_max_2  = filter(row -> Year(row[:date]) == Year(2019), x_max_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_3 = CSV.read(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy3hours.csv\")\n",
    "x_train_max_3 = filter(row -> Year(row[:date]) != Year(2019), x_max_3)\n",
    "x_pred_max_3  = filter(row -> Year(row[:date]) == Year(2019), x_max_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_4 = CSV.read(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy4hours.csv\")\n",
    "x_train_max_4 = filter(row -> Year(row[:date]) != Year(2019), x_max_4)\n",
    "x_pred_max_4  = filter(row -> Year(row[:date]) == Year(2019), x_max_4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_6 = CSV.read(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy6hours.csv\")\n",
    "x_train_max_6 = filter(row -> Year(row[:date]) != Year(2019), x_max_6)\n",
    "x_pred_max_6  = filter(row -> Year(row[:date]) == Year(2019), x_max_6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_8 = CSV.read(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy8hours.csv\")\n",
    "x_train_max_8 = filter(row -> Year(row[:date]) != Year(2019), x_max_8)\n",
    "x_pred_max_8  = filter(row -> Year(row[:date]) == Year(2019), x_max_8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max_12 = CSV.read(\"./data/new_datasets/max_precipitation_day_split/maxPrecBy12hours.csv\")\n",
    "x_train_max_12 = filter(row -> Year(row[:date]) != Year(2019), x_max_12)\n",
    "x_pred_max_12  = filter(row -> Year(row[:date]) == Year(2019), x_max_12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding which variables to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = CSV.read(\"./data/new_datasets/surverse_list.csv\");\n",
    "x_train = createDataEx([x_train_max, x_train_sum, x_train_max_2, x_train_max_3, x_train_max_4, x_train_max_2], [\"max\", \"sum\", \"max2\", \"max3\", \"max4\"], x_train_max_2[:, :date])\n",
    "x_pred = createDataEx([x_pred_max, x_pred_sum, x_pred_max_2, x_pred_max_3, x_pred_max_4, x_pred_max_2], [\"max\", \"sum\", \"max2\", \"max3\", \"max4\"], x_pred_max_2[:, :date])\n",
    "dropmissing!(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouvrages qui nous interesse\n",
    "ouvrages = [\"3260-01D\", \"3350-07D\", \"4240-01D\", \"4350-01D\", \"4380-01D\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histogram(convert(Matrix, x_train[:, Not(:date)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that most data is skewed towards 0cm with some extreme cases over 2000cm (outliers)\n",
    "\n",
    "we need to preprocess the data with a function to reduce this gap\n",
    "\n",
    "$ f(x) = ln(x+1) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_values = DataFrame([log.(col.+1) for col = eachcol(x_train[:, Not(:date)])])\n",
    "display(histogram(convert(Matrix, x_values)))\n",
    "display(describe(x_values, :mean, :std, :min, :q25, :median, :q75, :max))\n",
    "x_train_log = log.(x_train[:, Not(:date)] .+ 1)\n",
    "x_pred_log = log.(x_pred[:, Not(:date)] .+ 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTICOLINEARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CUTOFF = 7\n",
    "\n",
    "@sk_import decomposition : PCA\n",
    "pca = PCA()\n",
    "x_train_matrix = pca.fit_transform(toArray(x_train))\n",
    "\n",
    "explained_variance = cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "Plots.plot(\n",
    "    1:length(pca.explained_variance_ratio_),\n",
    "    explained_variance,\n",
    "    ylims=(0, 1.0),\n",
    "    xlabel=\"Number of components\",\n",
    "    ylabel=\"Cumulative sum of explained Variance\"\n",
    ")\n",
    "display(\n",
    "    annotate!([(12.5, .4, \n",
    "        text(\"Cutoff at $(CUTOFF) variables which is $(round(explained_variance[CUTOFF], digits=5))% of the total variance\",10)\n",
    "    )])\n",
    ")\n",
    "\n",
    "x_train_pca = DataFrame(\n",
    "    date=x_train[!,:date],\n",
    "    x₁=x_train_matrix[:,1],\n",
    "    x₂=x_train_matrix[:,2],\n",
    "    x₃=x_train_matrix[:,3],\n",
    "    x₄=x_train_matrix[:,4],\n",
    "    x₅=x_train_matrix[:,5],\n",
    "    x₆=x_train_matrix[:,6],\n",
    "    x₇=x_train_matrix[:,7],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aiming for\n",
    "\n",
    "- 80% TRAIN\n",
    "- 20% TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: enable use of different datasets (PCA with and without log)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = y_train\n",
    "X = x_pred\n",
    "X_PCA = x_train_pca\n",
    "\n",
    "function dataSplit(ouvrage, X_train = X_PCA) #HERE\n",
    "    y = getOuvrage(Y, ouvrage)\n",
    "    y_train, y_test = splitdataframe(y, .80)\n",
    "\n",
    "    x_train = getDataFromDates(X_train, y_train.date)\n",
    "    y_train = getDataFromDates(y_train, x_train.date)\n",
    "\n",
    "    x_test = getDataFromDates(X_train, y_test.date)\n",
    "    y_test = getDataFromDates(y_test, x_test.date)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@sk_import metrics: f1_score;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_on_all_ouvrages(model_contructor::Function)\n",
    "    pred = []\n",
    "    test = []\n",
    "    for ouvrage in ouvrages\n",
    "        x_train, x_test, y_train, y_test = dataSplit(ouvrage)\n",
    "        \n",
    "        model = model_contructor(x_train, y_train)\n",
    "        \n",
    "        prediction = model.predict(toArray(x_test));\n",
    "        \n",
    "        push!(test, toArray(y_test))\n",
    "        push!(pred, prediction)\n",
    "    end\n",
    "    \n",
    "    pred = collect(Iterators.flatten(pred))\n",
    "    test = collect(Iterators.flatten(test))\n",
    "    return f1_score(pred,test)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sk_import linear_model: LogisticRegression\n",
    "function modeleLogistique(x_train, y_train) \n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"liblinear\",\n",
    "        random_state=234,\n",
    "    )\n",
    "    model.fit(toArray(x_train), toArray(y_train));\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele de l'arbre de Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sk_import tree: DecisionTreeClassifier\n",
    "function modeleArbreDecisif(x_train, y_train)\n",
    "    model = DecisionTreeClassifier(\n",
    "        random_state=234,\n",
    "    )\n",
    "    model.fit(toArray(x_train), toArray(y_train));\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Iterations=100\n",
    "display(\"$(Iterations)  iterations\")\n",
    "\n",
    "models = [\n",
    "    modeleLogistique,\n",
    "    modeleArbreDecisif,\n",
    "];\n",
    "\n",
    "# TODO -> VALIDATE DATASETS\n",
    "for m in models\n",
    "    \n",
    "    score=[]\n",
    "    for i = 1:Iterations\n",
    "        f1 = train_on_all_ouvrages(m);\n",
    "        push!(score,f1)\n",
    "    end\n",
    "    f1 = mean(score)\n",
    "    \n",
    "    f1 = round(f1, digits=4)\n",
    "    m = string(m)\n",
    "\n",
    "    display(\"f1 = $(f1) with $(m)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results show:\n",
    "\n",
    "Logistic = .69\n",
    "\n",
    "Trees = .61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place des valeurs cherchees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CSV.read(\"data/test.csv\")\n",
    "df = test[:, [:NO_OUVRAGE, :DATE]]\n",
    "rename!(df, :DATE=>:date)\n",
    "X = join(df, x_pred, on=:date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predictLogit(df, modeles)\n",
    "    dataframe = DataFrame(NO_OUVRAGE=df.NO_OUVRAGE, x₁=df.sumMcTavish,x₂=df.sumBellevue,x₃=df.sumAssomption,x₄=df.sumTrudeau,x₅=df.sumStHubert,x₆=df.maxMcTavish,x₇=df.maxBellevue,x₈=df.maxAssomption,x₉=df.maxTrudeau,x₁₀=df.maxStHubert)\n",
    "    predictions = []\n",
    "    for x in eachrow(dataframe)\n",
    "        modele = filter(row -> row.NO_OUVRAGE == x.NO_OUVRAGE, modeles).modele[1]\n",
    "        predictions = vcat(predictions, GLM.predict(modele, DataFrame(x[Not([:NO_OUVRAGE])])))\n",
    "    end\n",
    "    return round.(predictions)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predictTree(dataframe, modeles)\n",
    "    predictions = []\n",
    "    for x in eachrow(dataframe)\n",
    "        modele = filter(row -> row.NO_OUVRAGE == x.NO_OUVRAGE, modeles).modele[1]\n",
    "        predictions = vcat(predictions, modele.predict([toArray(x)]))\n",
    "    end\n",
    "    return round.(predictions)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsTree = predictTree(X, modelesTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsLogit = predictLogit(X, modelesLogit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nombre de diff entre Tree et Logit\n",
    "sum(abs.(predictionsTree-predictionsLogit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du fichier de prédictions pour soumettre sur Kaggle\n",
    "\n",
    "Dans ce cas-ci, nous prédirons une surverse avec une prediction logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du fichier sampleSubmission.csv pour soumettre sur Kaggle\n",
    "ID = test[:,:NO_OUVRAGE].*\"_\".*string.(test[:,:DATE])\n",
    "sampleSubmission = DataFrame(ID = ID, Surverse=predictionsTree)\n",
    "CSV.write(\"sampleSubmission.csv\",sampleSubmission)\n",
    "\n",
    "# Vous pouvez par la suite déposer le fichier sampleSubmission.csv sur Kaggle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
